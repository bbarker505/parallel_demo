{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Introduction To Parallel Processing in R\"\nauthor: \"Brittany Barker\"\ndate: \"`r format(Sys.Date(), tz = 'America/Los_Angeles')`\"\noutput:\n  html_document:\n    df_print: paged\n---\n<style type=\"text/css\">\nbody{ /* Normal  */\n      font-size: 14px;\n  }\np {line-height: 1.5em;}\n</style>\n\n\n\n## Parallel processing\n- **What?** Parallel processing is a type of computation in which many calculations or the execution of processes are carried out simultaneously\n- **Why?** Analayzing very large data sets and/or needing to run a computationally demanding analysis many times can be **very** slow\n- **How?** Until the late 2000â€™s parallel computing was mainly done on clusters of large numbers of single- or dual-CPU computers. Nowadays even laptops have 2 or 4 CPU cores, and servers with 8, 32 or more cores are common .\n\nIn this presentation, I will demonstrate the use and value of the `parallel`, `doParallel`, and `foreach` packages for running analyses in R in parallel.  The demo will center on analyzing very large spatial data sets, but parallel processing in R can be used for **ANY** type of data.  \n\n\n## Demo\n\nThe demo uses annual maximum temperature (Tmax) data from the [PRISM data base](https://prism.oregonstate.edu/). PRISM climate data are offered as daily, monthly, or annual rasters and for the 48-state United States. Analyzing PRISM rasters can be very computationally demanding because of their fine spatial resolution (4 km).  We will conduct two different analyses that examine how Tmax has changed in recent years and compare run times with and with out parallel processing.  \n\n\n\n### (1) County-level change in Tmax in July\n\n**Question: For each year between 2016 and 2019, which OR counties experienced the hottest July temps compared to the 30 year average?**  \n\nWe will use three data sets: \n\n  1) Tmax for July for 2016, 2017, 2018 and 2019 (4 rasters)\n  2) Average Tmax for July between 1981-2010 (1 raster)\n  3) U.S. county simple features (sf) object  \n  \n```{r, warning = FALSE, message = FALSE}\nlibrary(dplyr)\nlibrary(raster)\nlibrary(USAboundaries)\nlibrary(sf)\nlibrary(stringr)\n\n\n# Load Tmax raster data from file\nJul_fls <- list.files(\"./prism_data/july/\", \n                      pattern = glob2rx(\"*201*.bil$*\"), full.names = TRUE)\nJul_30yr <- raster(\"./prism_data/normals/PRISM_tmax_30yr_normal_4kmM2_07_bil.bil\") \n\n# Load county data, extract OR counties, and make into a SpatialPolygonsDataFrame\ncountiesOR <- us_counties() %>%\n  mutate(state_county = paste(state_name, name, sep = \"_\")) %>%\n  filter(state_name == \"Oregon\") %>%\n  sf::as_Spatial(.)\n```\n\nTake a peek at the raster data - there are 872,505 raster cell values\n```{r}\nJul_30yr\n\nplot(Jul_30yr)\n```\n\nThe `top5_func()` function determines which 5 counties for any given year had the hottest July temps compared to the 30 year average. \n\n```{r}\ntop5_func <- function(x) {\n  rast <- raster(x)\n  # Subtract the 30 year average after converting both from DegC to DegF\n  anom_US <- (rast  * 1.8 + 32) - (Jul_30yr * 1.8 + 32)\n  # Calculated difference (anomoly) in Tmax\n  anom_cnty <- extract(anom_US, countiesOR, fun = mean, df = TRUE) # Summarizes raster values over counties\n  anom_cnty$county <- countiesOR$name\n  anom_cnty$mean_anom <- order(anom_cnty$layer)\n  # Estimate the average change in Tmax for each county\n  anom_cnty %>% \n    mutate(mean_anom = order(layer)) %>%\n    arrange(mean_anom) %>%\n    filter(mean_anom <= 5) %>%\n    dplyr::select(county, layer) %>% \n    rename(\"difference (F)\" = \"layer\")\n}\n```\n\n### Analyze each year sequentially\n\nThe list of Tmax rasters for each year may be analyzed sequentially using `lapply()` or a `for` loop\n```{r, warning = FALSE, message = FALSE}\n# Apply the function to the list of Tmax rasters\nsystem.time(\n  top5.1 <- lapply(Jul_fls, top5_func)\n)\n```\n\n### Analyze years in parallel\n\nRunning the analysis sequentially is pretty slow. Instead, Tmax data for each year can be anlayzed in parallel.  First, we need to create a parallel socket cluster (another word for core) using `parallel:makeCluster()`, which creates a user-specified number of copies of R to run in parallel. Be warned that specifying too many cores may overload your computer and potentially crash processes and programs (including R). \n\nSince only 4 Tmax data sets are being anlayzed, it is pointless to specify more than 4 cores. Let's compare run times using 2 vs. 4 cores.\n\n```{r}\nlibrary(parallel)\n\ncl.2 <- makeCluster(2)\ncl.4 <- makeCluster(4)\n```\n\nAt least two approaches may be used to run the analysis in parallel.    \n\n#### **Approach 1** \nA `foreach::foreach()` loop is essentially a hybrid of the standard `for()` loop and the `lapply()` function, and provides a looping construct for executing R code repeatedly in parallel. The `doParallel` package provides the backend to execute the `foreach()` loop.\n\n```{r, warning = FALSE, message = FALSE}\nlibrary(foreach)\nlibrary(doParallel)\n\n# Analysis using 2 cores\nregisterDoParallel(cl.2) # Register the parallel backend\nsystem.time(\n  # The required R packages must be specified in a foreach loop\n  top5.2 <- foreach(fl = Jul_fls, .packages = c(\"raster\", \"dplyr\"))\n  %dopar% { \n    top5_func(fl)\n  }\n)\n```\n\nYou should stop the clusters after you're done.\n\n```{r}\nstopCluster(cl.2)\n```\n\n````{r}\n# Analysis using 4 cores\nregisterDoParallel(cl.4)\nsystem.time(\n  top5.3 <- foreach(fl = Jul_fls, .packages = c(\"raster\", \"dplyr\"))\n  %dopar% { \n    top5_func(fl)\n  }\n)\n\nstopCluster(cl.4) # Stop cluster\n```\n\n\n#### **Approach 2** \nThe `parallel::mclapply()` function is analogous to `lapply()`, but it distributes the tasks to multiple processors. Unfortunately, `mclapply()` does not work on Windows machines because its implementation relies on forking and Windows does not support forking. \n\n```{r}\n\n# Analysis using 2 cores\n#registerDoParallel(cl.2) \nsystem.time(\n  top5.4 <- mclapply(Jul_fls, top5_func, mc.cores = 2)\n)\n\n```\n```{r}\n# Analysis using 4 cores\nsystem.time(\n  top5.5 <- mclapply(Jul_fls, top5_func, mc.cores = 4)\n)\n```\n\n### Results\nMultnomah county was in the list of top 5 counties which had a hotter than average July for two years in a row (2017 and 2018). It looks like July max temps across OR counties in 2016 and 2019 were in fact cooler than the 30 year average.\n\n```{r}\nnames(top5.1) <- as.character(2016:2019)\ntop5.1 \n```\n\n\n### (2) Changes in annual maximum temperatures in Oregon between 2000 and 2019  \n\n**Question: What was the trend in annual Tmax across Oregon in 1990-1999, 2000-2010, and 2011-2019?**    \n\n### Load, crop, and subset annual Tmax rasters\n\n```{r}\n# Load and crop Tmax data for entire U.S. for all years between 2000 and 2019\nUSA <- stack(list.files(path = paste0(getwd(), \"/prism_data/annual/\"),\n                        pattern = \".bil$*\", full.names = TRUE)) # A stack of rasters\nOR <- crop(USA, extent(-124.7294, -116.2949, 41.7150, 46.4612)) # Crop the raster stack\n\n# Separate rasters by decade (2000-2010 vs. 2011-2019) \nOR.90_99 <- OR[[1:10]]\nOR.00_10 <- OR[[11:21]] \nOR.11_19 <- OR[[22:30]]\nOR_sets <- list(OR.90_99, OR.00_10, OR.11_19)\n```\n\n\n### Slope function\n\nThe `slope_func()` function below calculates the slope at each raster grid cell, which provides insight into the direction of change in annual Tmax for each decade.\n\n```{r}\nslope_func <- function(r) {\n  years <- str_split_fixed(names(r), \"_\", 6)[,5]\n  if (all(is.na(r))) {\n    NA\n  } else {\n    m <- lm(r ~ years); summary(m)$coefficients[2] # Calculates the slope\n  }\n}\n```\n\n\n### Analyze trend of annual Tmax for each decade sequentially\n\n```{r}\nsystem.time(\n  slopes <- lapply(OR_sets, function(x) {\n    calc(x, slope_func)\n  })\n)\n```\n\n\n### Analyze trend of annual Tmax for decades in parallel\n\n#### **Approach 1** \nUsing a `foreach()` loop with 3 cores, since the analysis is being conducted on 3 data sets.\n\n```{r}\ncl.3 <- makeCluster(3)\nregisterDoParallel(cl.3)\n\nsystem.time(\n  slopes <- foreach(set = OR_sets, \n                    .packages = c(\"raster\", \"stringr\")) %dopar% { \n    calc(set, slope_func)\n  }\n)\n\nstopCluster(cl.3)\n```\n\n\n#### **Approach 2** \nUsing `mclapply()`\n\n```{r}\nsystem.time(\n  slopes <- mclapply(OR_sets, function(x) {\n    calc(x, slope_func)\n  }, mc.cores = 3)\n)\n```\n\n### Results\n\n```{r}\n# Look at the results\nnames(slopes[[1]]) <- c(\"dec.1990_1999\")\nnames(slopes[[2]]) <- c(\"dec.2000_2010\")\nnames(slopes[[3]]) <- c(\"dec.2011_2019\")\nplot(stack(slopes[[1]], slopes[[2]], slopes[[3]]))\n```\n\nIt looks like annual Tmax increased more in Central Oregon than along the Oregon coast over the last 20 years.\n\n### Wrap-up\nParallel processing in R can significantly reduce run times. Here we saw that run times were cut in half or two thirds depending on the analysis. Run times would be cut even further if I had analyzed more than 3 or 4 data sets. For example, analyzing county-level change in Tmax in July across counties of multiple U.S. states would take much longer. In this case, I could conduct parallel processing using even more cores, in addition to finding other potential tools to make processing more efficient. \n\n### Potentially useful vignettes, demos, and introductions include\n- The `multidplyr()` [package](https://rdrr.io/github/hadley/multidplyr/) partitions a data frame across multiple worker processes to provide simple multicore parallelism  \n- A `foreach()` [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html)   \n- R-bloggers demo [Let's be Faster and more Parallel in R with doParallel package](https://www.r-bloggers.com/lets-be-faster-and-more-parallel-in-r-with-doparallel-package/)  \n- [How to go parallel in R - basics + tips](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#The_foreach_package) with emphasis on `foreach()`  \n- Introduction to [Parallel computing in R](https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html)  \n- [Quick Intro to Parallel Computing in R](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html) using remote sensing data  \n- [Efficient Looping with R](http://ethen8181.github.io/Business-Analytics/R/efficient_looping/efficient_looping.html)  \n- [Examples of functions in the parallel package](https://www.r-bloggers.com/simple-parallel-processing-in-r)",
    "created" : 1596944794979.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1839032121",
    "id" : "433D7683",
    "lastKnownWriteTime" : 1597188079,
    "last_content_update" : 1597188079356,
    "path" : "~/parallel_demo/parallel_R.Rmd",
    "project_path" : "parallel_R.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}