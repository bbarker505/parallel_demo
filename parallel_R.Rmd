---
title: "Introduction To Parallel Processing in R"
author: "Brittany Barker"
date: "`r format(Sys.Date(), tz = 'America/Los_Angeles')`"
output:
  html_document:
    df_print: paged
---

```{=html}
<style type="text/css">
body{ /* Normal  */
      font-size: 14px;
  }
p {line-height: 1.5em;}
</style>
```

## Parallel processing

-   **What?** Parallel processing is a type of computation in which many calculations or the execution of processes are carried out simultaneously
-   **Why?** Analyzing very large data sets and/or needing to run a computationally demanding analysis many times can be **very** slow
-   **How?** Until the late 2000â€™s parallel computing was mainly done on clusters of large numbers of single- or dual-CPU computers. Nowadays even laptops have 2 or 4 CPU cores, and servers with 8, 32 or more cores are common.

## Operating system

-   Your OS and number of CPU cores will affect the speed and capabilities of parallel processing
-   To check number of cores in Windows, go into "System Information" (shortcut = Windwows+R and type "msinfo32"), and click on "Processor"
-   Multiple commands can check number of cores in Unix-based systems (e.g. Linux)
-   In R, the `detectCores()` function of the `parallel` package returns the number of cores of your OS

```{r, warning = FALSE, message = FALSE}
library(parallel)

detectCores()
```

## Purpose of this demo

In this demo, I will demonstrate the use and value of R packages including `parallel`, `doParallel`, `foreach` and `furrr` for running analyses in parallel. The demo will center on analyzing very large spatial data sets, but parallel processing in R can be used for **ANY** type of data. We will see how changing the number of cores used for parallel processing affects run times, and how your OS (Windows vs. Unix-based OS) affects processing capabilities.

## Demo

The demo uses annual maximum temperature (Tmax) data from the [PRISM data base](https://prism.oregonstate.edu/). PRISM climate data are offered as daily, monthly, or annual rasters for the 48-state United States. Analyzing PRISM rasters can be very computationally demanding because of their fine spatial resolution (4 km), particularly for large regions. We will conduct two different analyses that examine how Tmax has changed in recent years and compare run times with and with out parallel processing.

### (1) County-level change in Tmax in July

**Question: For each year between 2020 and 2024, which OR counties experienced the hottest July temps compared to the 30 year average?**

We will use three data sets:

1)  Tmax for July for 2020, 2021, 2022, 2023 and 2024 (5 rasters)
2)  Average Tmax for July between 1981-2010 (1 raster)
3)  U.S. county simple features (sf) object

```{r, warning = FALSE, message = FALSE}
library(dplyr)
library(tibble)
library(knitr)
library(terra)
library(USAboundaries)
library(sf)
library(stringr)

#install.packages("USAboundariesData", repos = "http://packages.ropensci.org", type = #"source")

# Load Tmax raster data from file
# July files for 2020-2024
Jul_fls <- list.files("./prism_data/july/", 
                      pattern = glob2rx("*202*.tif$*"), full.names = TRUE)
# July - 30 yr avg
Jul_30yrAv_fl <- "./prism_data/normals/PRISM_tmax_30yr_normal_4kmM2_07_bil.bil"

# Load county data, extract OR counties, and make into a SpatialPolygonsDataFrame
countiesOR <- us_counties() %>%
  mutate(state_county = paste(state_name, name, sep = "_")) %>%
  filter(state_name == "Oregon") 
```

Take a peek at the raster data - there are 872,505 raster cell values

```{r}
Jul_30yrAv <- rast(Jul_30yrAv_fl)

plot(Jul_30yrAv)
```

The `top5_func` function determines which 5 counties for any given year had the largest difference in July temperature compared to the 30 year average.

```{r}
top5_func <- function(i) {
  
  # Get rasters
  r <- rast(Jul_fls[i])
  r_avg <- rast(Jul_30yrAv_fl)
  
  # Subtract the 30 year average after converting both from DegC to DegF
  anom_US <- (r * 1.8 + 32) - (r_avg * 1.8 + 32)
  names(anom_US) <- "value"
  
  # Calculated difference (anomaly) in Tmax at each grid cell, and then 
  # calculate the average for each county
  anom_cnty <- suppressWarnings(extract(anom_US, countiesOR, fun = mean))
  anom_cnty$county <- countiesOR$name
  
  # Sort the results and extract the 5 counties with the largest anomalies
  anom_cnty <- anom_cnty %>% 
    mutate(difference_F = value,
           difference_F_abs = abs(difference_F),
           county = countiesOR$name) %>%
    arrange(desc(difference_F_abs)) %>%
    top_n(5) %>%
    select(county, difference_F)
  
  return(anom_cnty)
}
```

### Analyze each year sequentially

The list of Tmax rasters for each year may be analyzed sequentially using `lapply` or a `for` loop.

```{r, warning = FALSE, message = FALSE}
# Apply the function to the list of Tmax rasters
top5_sequential <- system.time(
  lapply(1:length(Jul_fls), function(i) {
    top5_func(i)
}))

top5_sequential
```

### Analyze years in parallel

Running the analysis sequentially is pretty slow. Instead, Tmax data for each year can be analyzed in parallel.

First, we need to create a parallel socket cluster (another word for core) using `parallel::makeCluster`, which creates a user-specified number of copies of R to run in parallel. WARNING: specifying too many cores may overload your computer and potentially crash processes and programs (including R).

Since only 4 Tmax data sets are being analyzed, it is pointless to specify more than 4 cores. Let's compare run times using 2 vs. 4 cores.

```{r}
library(parallel)
cl.2 <- makeCluster(2)
cl.4 <- makeCluster(4)
```

At least two approaches may be used to run the analysis in parallel.

#### **Approach 1:** `foreach`

A `foreach::foreach` loop is essentially a hybrid of the standard `for` loop and the `lapply` function, and provides a looping construct for executing R code repeatedly in parallel. The `doParallel` package provides the backend to execute the `foreach` loop.

Comparing a `foreach` loop to a `for` loop\
- The `%dopar%` function exports the tasks to parallel execution workers\
- A `foreach` loop returns a list of results by default (the `.combine` option can be used to specify how results are returned)\
- The R packages needed to complete the work must be specified in a `foreach` loop\
- Objects in a `foreach` loop are not in the global environment unless you export them

```{r, warning = FALSE, message = FALSE}
library(foreach)
library(doParallel)

# Analysis using 2 cores
registerDoParallel(cl.2) # Register the parallel backend
top5_foreach_cl2 <- system.time(
  
  # The required R packages must be specified in a foreach loop
  foreach(i = 1:length(Jul_fls), .packages = c("terra", "dplyr")) %dopar% {
    top5_func(i)
  
  })

# Run time - 2 cores
top5_foreach_cl2
```

You should stop the clusters after you're done.

```{r}
stopCluster(cl.2)
```

Do the same thing but with 4 cores.

```{r}
# Analysis using 4 cores
registerDoParallel(cl.4)

top5_foreach_cl4 <- system.time(
  top5.3 <- foreach(i = 1:length(Jul_fls), .packages = c("terra", "dplyr"))
  %dopar% { 
    top5_func(i)
  }
)

# Run time - 4 cores
top5_foreach_cl4

stopCluster(cl.4) # Stop cluster
```

#### **Approach 2**: `mclapply`

The `parallel::mclapply` function is analogous to `lapply`, but it distributes the tasks to multiple processors.

Unfortunately, `mclapply` does not work on Windows machines because its implementation relies on forking and Windows does not support forking. "Fork" is the name of the Unix call that the parent process uses to "divide" itself ("fork") into two identical processes that run independently from each other. Windows uses a threading module for multiprocessing, meaning that the process shares memory and resources.

By default, `mclapply` will use all cores available to it. You can specify the number of cores to use with the `mc.cores` option.

```{r}
if (!grepl("Windows", Sys.info()[1])) {
   # Analysis using 2 cores
  top5_mclapply_cl2 <- system.time(
    top5.4 <- mclapply(1:length(Jul_fls), function(i) {
      top5_func(i) 
      }, mc.cores = 2)
  )
  
  # Run time - 2 cores
  top5_mclapply_cl2 
}

```

Do the same thing but with 4 cores.

```{r}
if (!grepl("Windows", Sys.info()[1])) {
  # Analysis using 4 cores
  top5_mclapply_cl4 <- system.time(
    top5.5 <- mclapply(1:length(Jul_fls), function(i) {
      top5_func(i) 
    }, mc.cores = 4)
  )
  
  # Run time - 4 cores
  top5_mclapply_cl4
}
```

#### **Approach 3**: `future_map`

The `furrr` package in part of the tidyverse, a large collection of R packages designed for data science. It provides a way to perform parallel processing using the `purrr` mapping functions (e.g., `map`) by integrating with the `future` package, which manages parallel execution. Here, you need to implement the `multisession` strategy in the `plan()` function and define the number of parallel "workers" (i.e., cores). The `future_map` function runs our custom function in parallel for the various rasters.

```{r}
library(furrr)
library(future)

# Plan the future
plan(multisession, workers = 2)

# Run it using 2 workers
top5_furrr_cl2 <- system.time(
  top5.6 <- future_map(1:length(Jul_fls), function(i) top5_func(i))
)

# Run time - 2 cores
top5_furrr_cl2
```

Do the same thing but with 4 workers.

```{r}
# Plan the future
plan(multisession, workers = 4)

# Run it using 2 workers
top5_furrr_cl4 <- system.time(
  top5.7 <- future_map(1:length(Jul_fls), function(i) top5_func(i))
)

# Run time - 4 cores
top5_furrr_cl4
```

### Compare run times for top 5 county analysis

Notice how parallel processing using 4 cores is twice as fast as sequential processing. Run times for the analysis conducted in a `foreach` loop vs. with `mclapply` are very similar, although `mclapply` may be marginally faster.

```{r}
# Compare run times
# If using a Unix-based system (able to run mclapply)
if (!grepl("Windows", Sys.info()[1])) {
  all_times1 <- data.frame(
    rbind(top5_sequential, top5_foreach_cl2, top5_foreach_cl4, top5_mclapply_cl2, 
          top5_mclapply_cl4, top5_furrr_cl2, top5_furrr_cl4)) 
# Else on Windows (no mclapply)
} else {
    all_times1 <- data.frame(
    rbind(top5_sequential, top5_foreach_cl2, top5_foreach_cl4, 
          top5_furrr_cl2, top5_furrr_cl4)) 
}

all_times1 <- all_times1 %>%
    rownames_to_column(var = "run") %>%
    select(run, elapsed)

knitr::kable(all_times1)
```

### Top 5 county results and takeaway

The output below shows which 5 OR counties had the largest difference (anomaly) in temperature (F) from the 30 year average for July. The largest anomalies are for `2021`, a year with record-breaking heat for Oregon. Results for each year were generated much quicker by running July Tmax data for each year in parallel in some cases.

```{r}
cl.4 <- makeCluster(4)
registerDoParallel(cl.4)

top5 <- foreach(i = 1:length(Jul_fls), .packages = c("terra", "dplyr")) %dopar% { 
    top5_func(i)
  }

# Run time - 4 cores
names(top5) <- as.character(2020:2024)
top5
```

### (2) Changes in annual maximum temperatures in Oregon between 1990 and 2019

**Question: What was the trend in annual Tmax across Oregon for each decade between 1990 and 2019?**

### Get list of Tmax rasters file to analyze

```{r}
# Load and crop Tmax data files for entire U.S. for all years between 1990 and 2019
USA_fls <- list.files(path = paste0(getwd(), "/prism_data/annual/"),
                      pattern = ".bil$*", full.names = TRUE)
```

### Slope function

The `slope_func` function below loads, crops, and subsets annual Tmax rasters and then runs a regression model for each raster grid cell. The results will provide insight into the direction of change in annual Tmax for each decade.

```{r}
slope_func <- function(years) {
  
  # Get rasters and crop to OR, and retain layers of corresponding years
  USA_r <- rast(USA_fls)
  OR_r <- crop(USA_r, ext(-124.7294, -116.2949, 41.7150, 46.4612)) 
  OR_r <- OR_r[[years]]
  
  # Run a regression model for each cell of the raster stack
  m <- wrap(regress(OR_r, 1:nlyr(OR_r), na.rm = TRUE))
  return(m)
}
```

### Analyze trend of annual Tmax for each decade sequentially.

```{r, warning=FALSE}
year_sets <- list(c(1:10), c(11:20), c(21:30))
trend_sequential <- system.time(
  slopes <- lapply(year_sets, function(years) {
    slope_func(years)
  })
)

trend_sequential
```

### Analyze trend of annual Tmax for decades in parallel

#### **Approach 1**: `foreach`

Using a `foreach` loop with 3 cores, since the analysis is being conducted on 3 data sets.

```{r}
cl.3 <- makeCluster(3)
registerDoParallel(cl.3)

trend_foreach <- system.time(
  slopes <- foreach(years = year_sets, 
                    .packages = c("terra", "stringr")) %dopar% { 
    slope_func(years)                  
  }
)

trend_foreach

stopCluster(cl.3)
```

#### **Approach 2**: `mclapply`

Using `mclapply`.

```{r}
if (!grepl("Windows", Sys.info()[1])) {
  trend_mclapply <- system.time(
    slopes <- mclapply(year_sets, function(years) {
      slope_func(years)
      }, mc.cores = 3)
    )

  trend_mclapply
}
```

#### **Approach 3**: `future_map`

Using `future_map` in the `furrr` package.

```{r}
# Plan the future
plan(multisession, workers = 3)

trend_furrr <- system.time(
  slopes <- future_map(year_sets, function(years) {
    slope_func(years)
    }))

trend_furrr
```

### Tmax trend results and takeaway

We see run times improvements in using parallel processing in some cases. 

```{r}
# Compare run times
# If using UNIX-based machine
if (!grepl("Windows", Sys.info()[1])) {
  all_times2 <- data.frame(rbind(
    trend_sequential, trend_foreach, trend_mclapply, trend_furrr))
  # Else on Windows (no mclapply)
} else {
   all_times2 <- data.frame(rbind(
    trend_sequential, trend_foreach, trend_furrr)) 
}

all_times2 <- all_times2 %>% 
  rownames_to_column(var = "run") %>%
  select(run, elapsed)

knitr::kable(all_times2)
```

Let's look at the results. Annual Tmax exhibited different trends throughout OR if we look at each decade separately, but temperatures are generally increasing in most areas.

```{r}
# Calculate the slopes again
cl.3 <- makeCluster(3)
registerDoParallel(cl.3)

slopes <- foreach(years = year_sets, .packages = c("terra", "stringr")) %dopar% { 
    slope_func(years)                  
  }

stopCluster(cl.3)

# Unwrap rasters and name them
slopes_90_99 <- unwrap(slopes[[1]])$x
names(slopes_90_99) <- "OR_1990_1999"
slopes_00_10 <- unwrap(slopes[[2]])$x
names(slopes_00_10) <- c("OR_2000_2010")
slopes_11_20 <- unwrap(slopes[[3]])$x
names(slopes_11_20) <- c("OR_2011_2020")

# Look at the trend results
plot(c(slopes_90_99[[1]], slopes_00_10[[1]], slopes_11_20[[1]]),
     plg=list(title="Slope"))
```

### Wrap-up

Parallel processing in R can significantly reduce run times\
- In this demo, run times were cut in half or two thirds depending on the analysis\
- Time savings between sequential vs. parallel computations would be even greater if we had analyzed even more data sets in parallel\
- For example, let's say we had to analyze changes in Tmax in July across all 3,006 U.S. counties!\
- Access to servers with numerous cores (24, 48, etc.) may be essential

Consider using parallel processing in R when\
- You need to implement repetitive tasks\
- There is no other way to speed up computations\
- You have access to enough cores to make a notable difference in speed

It might not be worth it when\
- It's overloading your computer and causing programs to stall or crash\
- This may happen if your data set is extremely large, such that using even 2 or 3 cores is too much\
- You're able to speed up an analysis by other means (e.g. a more efficient function)

### Potentially useful vignettes, demos, and introductions

-   The `multidplyr` [package](https://rdrr.io/github/hadley/multidplyr/) partitions a data frame across multiple worker processes to provide simple multicore parallelism\
-   A `foreach` [vignette](https://cran.r-project.org/web/packages/foreach/vignettes/foreach.html)\
-   R-bloggers demo [Let's be Faster and more Parallel in R with doParallel package](https://www.r-bloggers.com/lets-be-faster-and-more-parallel-in-r-with-doparallel-package/)\
-   [How to go parallel in R - basics + tips](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#The_foreach_package) with emphasis on `foreach()`\
-   Introduction to [Parallel computing in R](https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html)\
-   [Quick Intro to Parallel Computing in R](https://nceas.github.io/oss-lessons/parallel-computing-in-r/parallel-computing-in-r.html) using remote sensing data\
-   [Efficient Looping with R](http://ethen8181.github.io/Business-Analytics/R/efficient_looping/efficient_looping.html)\
-   [Examples of functions in the parallel package](https://www.r-bloggers.com/simple-parallel-processing-in-r)
